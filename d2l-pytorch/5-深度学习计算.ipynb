{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fc68b1e-8456-4727-802a-46e9367cb650",
   "metadata": {},
   "source": [
    "第五章主要讲解模型的构建、模型参数的访问与初始化、设计自定义层和块、保存模型与加载模型以及使用GPU加速"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea616a9-e6f2-4161-987f-3e095b1697a1",
   "metadata": {},
   "source": [
    "# 5.0 查看网络结构的两种方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6933a71f-eb70-4c18-8632-7271b0f94aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        # 使用MLP的父类进行必要的初始化,(根据需要初始化模型)\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)\n",
    "        self.out = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 将输入数据作为前向传播的参数\n",
    "        out = self.hidden(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.out(out)\n",
    "        out = F.softmax(out, 1)\n",
    "        # 通过前向传播生成输出\n",
    "        return out\n",
    "MLP_NET = MLP()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f90549-7324-4991-93bd-73b31a1694ce",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "法一:使用print()函数打印网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6a0dd906-639a-4c8b-841b-e2f317c969df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (hidden): Linear(in_features=20, out_features=256, bias=True)\n",
      "  (out): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(MLP_NET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c042f2a-00cd-4859-892f-6090ad7ecb3a",
   "metadata": {},
   "source": [
    "法二:使用torchsummary库中Summary方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0709ca77-0e6f-4f3f-91d7-a9dd37fb71e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1               [-1, 1, 256]           5,376\n",
      "            Linear-2                [-1, 1, 10]           2,570\n",
      "================================================================\n",
      "Total params: 7,946\n",
      "Trainable params: 7,946\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.03\n",
      "Estimated Total Size (MB): 0.03\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(MLP_NET, (1, 20),device=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b88e1a-a282-4d3d-b603-9dbcc73f3a77",
   "metadata": {},
   "source": [
    "# 5.1 层和块"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544e0ff1-191b-42ce-9f68-dd13a91f7279",
   "metadata": {},
   "source": [
    "神经网络块：块（block）可以是单独的一层，也可以是由多个层组成的组件或者模型本身。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b170788-8b68-41f1-9efb-77d490837a21",
   "metadata": {},
   "source": [
    "## 5.1.1 自定义块"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6484a97-cc22-4ed2-a741-6227007802ce",
   "metadata": {},
   "source": [
    "每一个自定义块必须提供的基本功能：\n",
    "1. 输入数据作为前向传播方法的参数\n",
    "2. 通过前向传播方法生成输出\n",
    "3. 计算其输出关于输入的梯度，这个可通过其反向传播函数进行访问。\n",
    "4. 存储和访问前向传播计算所需的参数\n",
    "5. 根据需要初始化模型 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77fc1119-53bd-44bc-9e17-ce712738021f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1392, 0.0645, 0.1064, 0.1220, 0.1044, 0.1012, 0.0886, 0.1064, 0.0608,\n",
       "         0.1065]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        # 使用MLP的父类进行必要的初始化,(根据需要初始化模型)\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)\n",
    "        self.out = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 将输入数据作为前向传播的参数\n",
    "        out = self.hidden(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.out(out)\n",
    "        out = F.softmax(out, 1)\n",
    "        # 通过前向传播生成输出\n",
    "        return out\n",
    "X = torch.randn(1, 20)\n",
    "MLP_NET = MLP()\n",
    "out = MLP_NET(X)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2284499-f20d-4f4b-bf2e-5c4842fd3cb2",
   "metadata": {},
   "source": [
    "## 5.1.2 顺序块"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a49f79-173e-45f7-960e-54bc192494e3",
   "metadata": {},
   "source": [
    "讲解如何构建Squqential类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d41ea2-d80a-4674-a9b5-b13b126411e2",
   "metadata": {},
   "source": [
    "构建简化的Sequential类,只需要定义两个关键的方法:\n",
    "1. 将块逐个追加到列表中的方法\n",
    "2. 前向传播方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a6a69295-4acf-4b00-8b57-4b147adb7f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MySequential(\n",
      "  (0): Linear(in_features=20, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=256, out_features=10, bias=True)\n",
      "  (3): Softmax(dim=1)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.1137, 0.1208, 0.1127, 0.1065, 0.0688, 0.0893, 0.1104, 0.0822, 0.1098,\n",
       "         0.0859]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        # enumerate()函数用于将可迭代对象组合为一个索引序列\n",
    "        for idx, module in enumerate(args):\n",
    "            # _modules,继承父类,父类定义的一个有序字典,保证每个添加的块都按照被添加的顺序执行,同时保证在初始化过程中,系统在_modules字典中查找需要初始化的参数\n",
    "            self._modules[str(idx)] = module\n",
    "\n",
    "    def forward(self, x):\n",
    "        for block in self._modules.values():\n",
    "            x = block(x)\n",
    "        return x\n",
    "MySequential_NET = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10), nn.Softmax(dim=1))\n",
    "print(MySequential_NET)\n",
    "x = torch.randn(1, 20)\n",
    "out = MySequential_NET(x)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8854160-7be7-45af-ae2f-a173142b7d97",
   "metadata": {},
   "source": [
    "## 5.1.3 在前向传播中执行代码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb67955-3e6c-4e76-9362-00ef7bc0d273",
   "metadata": {},
   "source": [
    "告诉我们可以在前向传播中加入其他的任意代码,可以是Python的控制流程,也可以是任意的数学运算等等\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3426658e-66b8-4695-99cd-2e6860092088",
   "metadata": {},
   "source": [
    "**注意:在前向传播中进行非线性变换,必须使用torh.nn.functional中的非线性变换函数,否则回产生\"TypeError: linear(): argument 'input' (position 1) must be Tensor, not ReLU\"错误,nn中的非线性变化函数用于块中**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7fbc31a9-7d17-46b3-b8c1-0515254a3113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4603, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class FixeHiddenMLP(nn.Module):  \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rand_c = torch.rand((20, 20), requires_grad=False)\n",
    "        self.linear = nn.Linear(20, 20)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        # 常量参数参与计算\n",
    "        x = F.relu(torch.mm(x, self.rand_c) + 1)\n",
    "        # 复用全连接层,相当于两个全连接层共享参数\n",
    "        x = self.linear(x)\n",
    "        # 在前向传播中加入控制流,注此操作可能不会用于实际任务\n",
    "        while x.abs().sum() > 1:\n",
    "            x /= 2\n",
    "        return x.sum()\n",
    "        \n",
    "x = torch.randn((1, 20))\n",
    "FixeHiddenMLP_Net = FixeHiddenMLP()\n",
    "out = FixeHiddenMLP_Net(x)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfcf14a-5bee-4da3-8f96-76c0cf125eb8",
   "metadata": {},
   "source": [
    "可以使用nn.Sequential()混搭各种组合块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b3badf46-e64b-408f-b56f-4b9317f820d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): NestMLP(\n",
      "    (net): Sequential(\n",
      "      (0): Linear(in_features=20, out_features=64, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=64, out_features=32, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (linear): Linear(in_features=32, out_features=16, bias=True)\n",
      "  )\n",
      "  (1): Linear(in_features=16, out_features=20, bias=True)\n",
      "  (2): FixeHiddenMLP(\n",
      "    (linear): Linear(in_features=20, out_features=20, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-0.0997, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(20, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.linear = nn.Linear(32, 16)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "x = torch.rand((1, 20))\n",
    "chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixeHiddenMLP())\n",
    "print(chimera)\n",
    "chimera(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2ece4d-62e5-43f5-917e-ae8c2f0edd90",
   "metadata": {},
   "source": [
    "## 小结"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17ec849-b1b2-4aa6-85ba-ca88041787db",
   "metadata": {},
   "source": [
    "+ 一个块可以由许多层组成;一个块可以由多个块组成\n",
    "+ 层和块,块和块之间的顺序连接由Sequential()类处理\n",
    "+ 可以在前向传播中加入任意的控制代码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1908c4-b6b6-4afd-854a-075d44ad0380",
   "metadata": {},
   "source": [
    "## 练习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbd81b2-d91f-4af6-bd2b-239da2b5de46",
   "metadata": {},
   "source": [
    "1. 如果将MySequential中存储块的方式更改为Python列表，会出现什么样的问题？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "491476b8-a806-4937-8898-277bbe73b1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class ListSequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        self.module_lsit = list(args)\n",
    "    def forward(self, x):\n",
    "        for block in self.module_lsit:\n",
    "            x = block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9fc71da3-a7fb-412e-ab64-83b9890671b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModuleSequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            self._modules[str(idx)] = module\n",
    "    def forward(self, x):\n",
    "        for block in self._modules.values():\n",
    "            x = block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0916bfd5-c49f-4dd1-b59f-5609b698a458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ListSequential_NET: ListSequential()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0035, -0.0280, -0.0672, -0.2855, -0.1194, -0.1932,  0.1229, -0.1675,\n",
       "         -0.1212,  0.2715]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 存储块的方式为list\n",
    "x = torch.rand((1, 20))\n",
    "ListSequential_NET = ListSequential(nn.Linear(20, 156), nn.ReLU(), nn.Linear(156, 10))\n",
    "print(\"ListSequential_NET:\", ListSequential_NET)\n",
    "ListSequential_NET(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c79254d9-30b8-42b0-9208-886c87f0b285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleSequential_NET: ModuleSequential(\n",
      "  (0): Linear(in_features=20, out_features=156, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=156, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2038,  0.0299, -0.1446,  0.0459,  0.0473, -0.2123, -0.1981,  0.3035,\n",
       "          0.3139, -0.0897]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 存储块的方式为_modules\n",
    "x = torch.rand((1, 20))\n",
    "ModuleSequential_NET = ModuleSequential(nn.Linear(20, 156), nn.ReLU(), nn.Linear(156, 10))\n",
    "print(\"ModuleSequential_NET:\", ModuleSequential_NET)\n",
    "ModuleSequential_NET(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8b642f-5d22-4151-9f15-ae70176d0cf2",
   "metadata": {},
   "source": [
    "**通过对比可以发现,使用List进行存储块并不进行正常的使用;但是无法打印网络结构,这是因为相较与默认位置(_modules)存储的网络,自定义位置存储的网络没有\"注册\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "595905e4-f0ae-49b9-9ca1-9eb7919ee4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "Error: 'int' object has no attribute 'numpy'\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "try:\n",
    "    summary(ListSequential_NET, (1, 20), device=\"cpu\")\n",
    "except Exception as exc:\n",
    "    print(\"Error:\", exc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53876dbb-2543-4c24-b5fa-5da6257e837d",
   "metadata": {},
   "source": [
    "**通过测试发现存储在自定义位置上的网络,也不能使用summary()函数查看网络结构**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda58798-967f-4fb7-8a96-8345869bca3e",
   "metadata": {},
   "source": [
    "2. 实现一个块，它以两个块为参数，例如net1和net2，并返回前向传播中两个网络的串联输出。这也被称为平行块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a5139f69-0c74-43c0-9e3b-d23161312474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block_NET: Block(\n",
      "  (block): Sequential(\n",
      "    (0): Block1(\n",
      "      (block1): Sequential(\n",
      "        (0): Linear(in_features=20, out_features=256, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block2(\n",
      "      (block2): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=256, out_features=10, bias=True)\n",
      "        (3): ReLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0378, 0.0195, 0.0119, 0.0000, 0.0000, 0.0499, 0.0000, 0.0000, 0.1264,\n",
       "         0.0000]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class Block1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Linear(20, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "class Block2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x = self.block2(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, block1:nn.Module, block2:nn.Module):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            block1,\n",
    "            block2\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.block(x)\n",
    "        return x\n",
    "x = torch.rand((1, 20))\n",
    "Block_NET = Block(Block1(), Block2())\n",
    "print(\"Block_NET:\", Block_NET)\n",
    "Block_NET(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f140bc0e-6901-4f29-a5bd-8afcb6a140af",
   "metadata": {},
   "source": [
    "3. 假设我们想要连接同一网络的多个实例。实现一个函数，该函数生成同一个块的多个实例，并在此基础上构建更大的网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "89b62f6b-fbf4-44cb-be46-f2768b52e68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 无意义,维度不匹配"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
