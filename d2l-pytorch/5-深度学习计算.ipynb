{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fc68b1e-8456-4727-802a-46e9367cb650",
   "metadata": {},
   "source": [
    "第五章主要讲解模型的构建、模型参数的访问与初始化、设计自定义层和块、保存模型与加载模型以及使用GPU加速"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea616a9-e6f2-4161-987f-3e095b1697a1",
   "metadata": {},
   "source": [
    "# 5.0 查看网络结构的两种方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "6933a71f-eb70-4c18-8632-7271b0f94aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        # 使用MLP的父类进行必要的初始化,(根据需要初始化模型)\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)\n",
    "        self.out = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 将输入数据作为前向传播的参数\n",
    "        out = self.hidden(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.out(out)\n",
    "        out = F.softmax(out, 1)\n",
    "        # 通过前向传播生成输出\n",
    "        return out\n",
    "MLP_NET = MLP()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f90549-7324-4991-93bd-73b31a1694ce",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "法一:使用print()函数打印网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "6a0dd906-639a-4c8b-841b-e2f317c969df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (hidden): Linear(in_features=20, out_features=256, bias=True)\n",
      "  (out): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(MLP_NET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c042f2a-00cd-4859-892f-6090ad7ecb3a",
   "metadata": {},
   "source": [
    "法二:使用torchsummary库中Summary方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "0709ca77-0e6f-4f3f-91d7-a9dd37fb71e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1               [-1, 1, 256]           5,376\n",
      "            Linear-2                [-1, 1, 10]           2,570\n",
      "================================================================\n",
      "Total params: 7,946\n",
      "Trainable params: 7,946\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.03\n",
      "Estimated Total Size (MB): 0.03\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(MLP_NET, (1, 20),device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b88e1a-a282-4d3d-b603-9dbcc73f3a77",
   "metadata": {},
   "source": [
    "# 5.1 层和块"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544e0ff1-191b-42ce-9f68-dd13a91f7279",
   "metadata": {},
   "source": [
    "神经网络块：块（block）可以是单独的一层，也可以是由多个层组成的组件或者模型本身。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b170788-8b68-41f1-9efb-77d490837a21",
   "metadata": {},
   "source": [
    "## 5.1.1 自定义块"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6484a97-cc22-4ed2-a741-6227007802ce",
   "metadata": {},
   "source": [
    "每一个自定义块必须提供的基本功能：\n",
    "1. 输入数据作为前向传播方法的参数\n",
    "2. 通过前向传播方法生成输出\n",
    "3. 计算其输出关于输入的梯度，这个可通过其反向传播函数进行访问。\n",
    "4. 存储和访问前向传播计算所需的参数\n",
    "5. 根据需要初始化模型 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fc1119-53bd-44bc-9e17-ce712738021f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        # 使用MLP的父类进行必要的初始化,(根据需要初始化模型)\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)\n",
    "        self.out = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 将输入数据作为前向传播的参数\n",
    "        out = self.hidden(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.out(out)\n",
    "        out = F.softmax(out, 1)\n",
    "        # 通过前向传播生成输出\n",
    "        return out\n",
    "X = torch.randn(1, 20)\n",
    "MLP_NET = MLP()\n",
    "out = MLP_NET(X)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2284499-f20d-4f4b-bf2e-5c4842fd3cb2",
   "metadata": {},
   "source": [
    "## 5.1.2 顺序块"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a49f79-173e-45f7-960e-54bc192494e3",
   "metadata": {},
   "source": [
    "讲解如何构建Squqential类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d41ea2-d80a-4674-a9b5-b13b126411e2",
   "metadata": {},
   "source": [
    "构建简化的Sequential类,只需要定义两个关键的方法:\n",
    "1. 将块逐个追加到列表中的方法\n",
    "2. 前向传播方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a69295-4acf-4b00-8b57-4b147adb7f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        # enumerate()函数用于将可迭代对象组合为一个索引序列\n",
    "        for idx, module in enumerate(args):\n",
    "            # _modules,继承父类,父类定义的一个有序字典,保证每个添加的块都按照被添加的顺序执行,同时保证在初始化过程中,系统在_modules字典中查找需要初始化的参数\n",
    "            self._modules[str(idx)] = module\n",
    "\n",
    "    def forward(self, x):\n",
    "        for block in self._modules.values():\n",
    "            x = block(x)\n",
    "        return x\n",
    "MySequential_NET = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10), nn.Softmax(dim=1))\n",
    "print(MySequential_NET)\n",
    "x = torch.randn(1, 20)\n",
    "out = MySequential_NET(x)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8854160-7be7-45af-ae2f-a173142b7d97",
   "metadata": {},
   "source": [
    "## 5.1.3 在前向传播中执行代码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb67955-3e6c-4e76-9362-00ef7bc0d273",
   "metadata": {},
   "source": [
    "告诉我们可以在前向传播中加入其他的任意代码,可以是Python的控制流程,也可以是任意的数学运算等等\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3426658e-66b8-4695-99cd-2e6860092088",
   "metadata": {},
   "source": [
    "**注意:在前向传播中进行非线性变换,必须使用torh.nn.functional中的非线性变换函数,否则回产生\"TypeError: linear(): argument 'input' (position 1) must be Tensor, not ReLU\"错误,nn中的非线性变化函数用于块中**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbc31a9-7d17-46b3-b8c1-0515254a3113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class FixeHiddenMLP(nn.Module):  \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rand_c = torch.rand((20, 20), requires_grad=False)\n",
    "        self.linear = nn.Linear(20, 20)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        # 常量参数参与计算\n",
    "        x = F.relu(torch.mm(x, self.rand_c) + 1)\n",
    "        # 复用全连接层,相当于两个全连接层共享参数\n",
    "        x = self.linear(x)\n",
    "        # 在前向传播中加入控制流,注此操作可能不会用于实际任务\n",
    "        while x.abs().sum() > 1:\n",
    "            x /= 2\n",
    "        return x.sum()\n",
    "        \n",
    "x = torch.randn((1, 20))\n",
    "FixeHiddenMLP_Net = FixeHiddenMLP()\n",
    "out = FixeHiddenMLP_Net(x)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfcf14a-5bee-4da3-8f96-76c0cf125eb8",
   "metadata": {},
   "source": [
    "可以使用nn.Sequential()混搭各种组合块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3badf46-e64b-408f-b56f-4b9317f820d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(20, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.linear = nn.Linear(32, 16)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "x = torch.rand((1, 20))\n",
    "chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixeHiddenMLP())\n",
    "print(chimera)\n",
    "chimera(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2ece4d-62e5-43f5-917e-ae8c2f0edd90",
   "metadata": {},
   "source": [
    "## 小结"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17ec849-b1b2-4aa6-85ba-ca88041787db",
   "metadata": {},
   "source": [
    "+ 一个块可以由许多层组成;一个块可以由多个块组成\n",
    "+ 层和块,块和块之间的顺序连接由Sequential()类处理\n",
    "+ 可以在前向传播中加入任意的控制代码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1908c4-b6b6-4afd-854a-075d44ad0380",
   "metadata": {},
   "source": [
    "## 练习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbd81b2-d91f-4af6-bd2b-239da2b5de46",
   "metadata": {},
   "source": [
    "1. 如果将MySequential中存储块的方式更改为Python列表，会出现什么样的问题？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491476b8-a806-4937-8898-277bbe73b1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class ListSequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        self.module_lsit = list(args)\n",
    "    def forward(self, x):\n",
    "        for block in self.module_lsit:\n",
    "            x = block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc71da3-a7fb-412e-ab64-83b9890671b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModuleSequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            self._modules[str(idx)] = module\n",
    "    def forward(self, x):\n",
    "        for block in self._modules.values():\n",
    "            x = block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0916bfd5-c49f-4dd1-b59f-5609b698a458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 存储块的方式为list\n",
    "x = torch.rand((1, 20))\n",
    "ListSequential_NET = ListSequential(nn.Linear(20, 156), nn.ReLU(), nn.Linear(156, 10))\n",
    "print(\"ListSequential_NET:\", ListSequential_NET)\n",
    "ListSequential_NET(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79254d9-30b8-42b0-9208-886c87f0b285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 存储块的方式为_modules\n",
    "x = torch.rand((1, 20))\n",
    "ModuleSequential_NET = ModuleSequential(nn.Linear(20, 156), nn.ReLU(), nn.Linear(156, 10))\n",
    "print(\"ModuleSequential_NET:\", ModuleSequential_NET)\n",
    "ModuleSequential_NET(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8b642f-5d22-4151-9f15-ae70176d0cf2",
   "metadata": {},
   "source": [
    "**通过对比可以发现,使用List进行存储块并不进行正常的使用;但是无法打印网络结构,这是因为相较与默认位置(_modules)存储的网络,自定义位置存储的网络没有\"注册\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "595905e4-f0ae-49b9-9ca1-9eb7919ee4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "Error: 'int' object has no attribute 'numpy'\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "try:\n",
    "    summary(ListSequential_NET, (1, 20), device=\"cpu\")\n",
    "except Exception as exc:\n",
    "    print(\"Error:\", exc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53876dbb-2543-4c24-b5fa-5da6257e837d",
   "metadata": {},
   "source": [
    "**通过测试发现存储在自定义位置上的网络,也不能使用summary()函数查看网络结构**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda58798-967f-4fb7-8a96-8345869bca3e",
   "metadata": {},
   "source": [
    "2. 实现一个块，它以两个块为参数，例如net1和net2，并返回前向传播中两个网络的串联输出。这也被称为平行块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5139f69-0c74-43c0-9e3b-d23161312474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class Block1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Linear(20, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "class Block2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x = self.block2(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, block1:nn.Module, block2:nn.Module):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            block1,\n",
    "            block2\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.block(x)\n",
    "        return x\n",
    "x = torch.rand((1, 20))\n",
    "Block_NET = Block(Block1(), Block2())\n",
    "print(\"Block_NET:\", Block_NET)\n",
    "Block_NET(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f140bc0e-6901-4f29-a5bd-8afcb6a140af",
   "metadata": {},
   "source": [
    "3. 假设我们想要连接同一网络的多个实例。实现一个函数，该函数生成同一个块的多个实例，并在此基础上构建更大的网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b62f6b-fbf4-44cb-be46-f2768b52e68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 无意义,维度不匹配"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b9ca04-932a-40e3-825e-e71600266b3c",
   "metadata": {},
   "source": [
    "# 5.2 参数管理 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031fc614-f458-4efb-9680-b60a909e3e71",
   "metadata": {},
   "source": [
    "本节主要介绍:\n",
    "+ 如何访问参数\n",
    "+ 如何初始化参数\n",
    "+ 如何在不同模型组件间共享参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5792b9-b03f-41aa-b79b-a582230fd71e",
   "metadata": {},
   "source": [
    "## 5.2.1 参数访问"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bd6b24-f427-4022-b61d-b31d8857ae6d",
   "metadata": {},
   "source": [
    "**使用state_dict()方法，获取模型参数字典,键为网络层名，值为其权重**\n",
    "\n",
    "**网络中每一个参数的参数名唯一**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bb9725-f7b1-4a22-ac49-0f7ab6bbd9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "param_net = nn.Sequential(\n",
    "    nn.Linear(4, 8),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(8, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d33976-deac-4af3-911c-404c45efa0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取模型全部参数\n",
    "print(\"All Param:\", param_net.state_dict())\n",
    "\n",
    "# 获取模型一层参数\n",
    "print(\"layer Param:\", param_net[2].state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016e6bd0-ca55-410a-b78d-043ffaae3ba0",
   "metadata": {},
   "source": [
    "+ 对参数进行任何操作，都需要访问到底层数值\n",
    "+ 参数是复合对象，包含值、梯度以及额外信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dea698-b273-44f9-8177-813ff765723e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取网络中偏置参数\n",
    "print(\"bias:\", param_net[0].bias)\n",
    "# 获取网络中偏置参数值\n",
    "print(\"bias value:\", param_net[0].bias.data)\n",
    "# 获取网络中偏置参数梯度信息,由于未进行反向传播，此时梯度处于初始化状态\n",
    "print(\"bias gradient:\", param_net[0].bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd3774d-f291-4fe9-b400-c8a8d0bc611b",
   "metadata": {},
   "source": [
    "**named_parameters()方法可以获取网络参数名称、即权重**\n",
    "\n",
    "**使用遍历的方法获取获取每一个块的参数**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b4d7dd-fb2c-42c0-bc1f-a780fa2f73e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class ParamBlock1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Linear(20, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "class ParamBlock2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x = self.block2(x)\n",
    "        return x\n",
    "\n",
    "class ParamBlock(nn.Module):\n",
    "    def __init__(self, block1:nn.Module, block2:nn.Module):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            block1,\n",
    "            block2\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.block(x)\n",
    "        return x\n",
    "ParamBlock_NET = ParamBlock(Block1(), Block2())\n",
    "ParamBlock_NET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb6ef5f-0951-4fcd-bdaf-832af7830f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 遍历获取名称及权重\n",
    "paramter = [(name, param.shape) for name, param in ParamBlock_NET.named_parameters()]\n",
    "\n",
    "# 使用state_dict()方法，通过网络层名称获取权重数据\n",
    "param_weight = ParamBlock_NET.state_dict()[\"block.0.block1.0.weight\"].data\n",
    "\n",
    "paramter, param_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ac792d-cbd0-46ac-8527-a4194d28d818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据网络层名称直接获取权重\n",
    "import torch.nn as nn\n",
    "def block1(grad=True):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(4, 8),\n",
    "        nn.Linear(8,4)\n",
    "    )\n",
    "\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4):\n",
    "        # add_module()方法向网络中添加新的块\n",
    "        net.add_module(f\"block{i}\", block1())\n",
    "    return net\n",
    "\n",
    "rgnet = nn.Sequential(block2(), nn.Linear(4, 1))\n",
    "print(\"rgnet network struct:\", rgnet)\n",
    "rgnet[0][2][1].weight,rgnet[0][2][1].bias.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e52fccc-4fd1-4f9e-a68a-01650173d2bf",
   "metadata": {},
   "source": [
    "## 5.2.2 参数初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4effcae-fe1c-4c0d-8af7-8323775d03a1",
   "metadata": {},
   "source": [
    "本节主要介绍：\n",
    "+ 使用Pytorch框架提供的初始化方法，初始化网络\n",
    "+ 创建自定义初始化方法，初始化网络\n",
    "\n",
    "**nn.init 模块中提供多种初始化方法**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79459b86-877b-431e-85d7-1d5e155ad3b2",
   "metadata": {},
   "source": [
    "### 内置模块的初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e33834-fd0b-40ad-bee8-c1cd2563dfe8",
   "metadata": {},
   "source": [
    "使用方法：\n",
    "1. 定义初始化方法\n",
    "    1. 选择初始化层\n",
    "    2. 定义初始化方法\n",
    "2. 使用网络的apply()方法，选择初始化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "b4a4d9c8-c922-4b1a-9d62-8303051a2292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['net.block0.0.weight', 'net.block0.0.bias', 'net.block0.1.weight', 'net.block0.1.bias', 'net.block1.0.weight', 'net.block1.0.bias', 'net.block1.1.weight', 'net.block1.1.bias', 'net.block2.0.weight', 'net.block2.0.bias', 'net.block2.1.weight', 'net.block2.1.bias'])\n",
      "init weight: tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "No init weight: tensor([[-0.4951,  0.0704, -0.3250, -0.4284],\n",
      "        [-0.3080,  0.2684,  0.1131, -0.0220],\n",
      "        [ 0.3174, -0.3203, -0.2856,  0.1511],\n",
      "        [ 0.1238, -0.1565, -0.0840,  0.0787],\n",
      "        [ 0.4437,  0.3488,  0.1358, -0.3189],\n",
      "        [ 0.2287, -0.3093,  0.3192,  0.4809],\n",
      "        [-0.0918,  0.1274, -0.4113,  0.1191],\n",
      "        [ 0.0545, -0.2957, -0.2685,  0.0310]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "class InitNet(nn.Module):\n",
    "    def __init__(self, init=False):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential()\n",
    "        for i in range(3):\n",
    "            self.net.add_module(f\"block{i}\", self.block())\n",
    "        if init is True:\n",
    "            self.net.apply(self.init_normal)\n",
    "            \n",
    "    def block(self):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(4, 8),\n",
    "            nn.Linear(8 ,4)\n",
    "        )\n",
    "    # 该初始化方法采用递归，而在python中，对递归层数是有限制（3000），所以当网络结构很深时，可能会递归层数过深的错误。（测试是没问题的）\n",
    "    def init_normal(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.zeros_(module.weight.data)\n",
    "            nn.init.normal_(module.bias.data, mean=1, std=2) \n",
    "            \n",
    "    def forworad(self, x):\n",
    "        return self.net(x)\n",
    "initNet = InitNet(init=True)\n",
    "print(initNet.state_dict().keys())\n",
    "print(\"init weight:\", initNet.state_dict()[\"net.block0.0.weight\"])\n",
    "NoinitNet = InitNet()\n",
    "print(\"No init weight:\", NoinitNet.state_dict()[\"net.block0.0.weight\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "9de88bc6-fb7e-4724-b1db-226131fc0c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init weight: tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "class InitNet(nn.Module):\n",
    "    def __init__(self, init=False):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential()\n",
    "        for i in range(3):\n",
    "            self.net.add_module(f\"block{i}\", self.block())\n",
    "        if init is True:\n",
    "            # 该方法为官方实例，使用遍历初始化网络\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    nn.init.zeros_(m.weight.data)\n",
    "                    nn.init.normal_(m.bias.data, mean=1, std=2)\n",
    "            \n",
    "    def block(self):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(4, 8),\n",
    "            nn.Linear(8 ,4)\n",
    "        )\n",
    "\n",
    "    def forworad(self, x):\n",
    "        return self.net(x)\n",
    "initNet = InitNet(init=True)\n",
    "print(\"init weight:\", initNet.state_dict()[\"net.block0.0.weight\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceefc4fd-3ded-4810-8be3-103626e569c5",
   "metadata": {},
   "source": [
    "### 自定义初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdd4cf6-1f63-449c-8435-adf359d67f9b",
   "metadata": {},
   "source": [
    "自定义初始化与内置初始化步骤完全相同，自定义初始化需要自己设置初始化规则"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bec41b5-1ce7-43ad-a2d2-16dd04730d7a",
   "metadata": {},
   "source": [
    "## 5.2.3 参数绑定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579fc920-e0bd-41bd-a851-9d7c4eb4e936",
   "metadata": {},
   "source": [
    "作用：实现多个层之间共享参数,在反向传播时共享参数的层的梯度会叠加"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e304562-91ca-4440-9429-215fc9372d79",
   "metadata": {},
   "source": [
    "实现：\n",
    "1. 在网络外创建一个共享层\n",
    "2. 将共享层添加到网络内"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "907a0e70-2822-41cc-a09f-ea74eda6493c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1769]], grad_fn=<AddmmBackward0>)\n",
      "tensor([True, True, True, True, True, True, True, True])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "share_layer = nn.Linear(8, 8)\n",
    "\n",
    "param_bind = nn.Sequential(\n",
    "    nn.Linear(4, 8),\n",
    "    nn.ReLU(),\n",
    "    share_layer,\n",
    "    nn.ReLU(),\n",
    "    share_layer,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(8, 1)\n",
    ")\n",
    "\n",
    "x = torch.rand((1, 4))\n",
    "print(param_bind(x))\n",
    "# 检查参数是否相同\n",
    "print(param_bind[2].weight.data[0] == param_bind[4].weight.data[0])\n",
    "# 检查类型是否相同\n",
    "print(type(param_bind[2].weight) == type(param_bind[4].weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a809550b-c993-4940-9bc4-7cfc93b7d57c",
   "metadata": {},
   "source": [
    "## 练习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e494fa0-4588-4f2b-8b12-5d6399947514",
   "metadata": {},
   "source": [
    "1. 使用 5.1节 中定义的FancyMLP模型，访问各个层的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "65ef5cb6-06d7-4783-a8ec-801f1d89ec95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class InitNet(nn.Module):\n",
    "    def __init__(self, init=False):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential()\n",
    "        for i in range(3):\n",
    "            self.net.add_module(f\"block{i}\", self.block())\n",
    "        if init is True:\n",
    "            # 该方法为官方实例，使用遍历初始化网络\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    nn.init.normal_(m.weight.data, mean=1, std=2)\n",
    "                    nn.init.zeros_(m.bias.data)\n",
    "            \n",
    "    def block(self):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(4, 8),\n",
    "            nn.Linear(8 ,4)\n",
    "        )\n",
    "\n",
    "    def forworad(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "c6cb0aad-37f3-4328-bb42-7fdf31a0b589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net.block0.0.weight tensor([[ 2.1666,  4.0705,  0.6399,  0.3653],\n",
      "        [ 3.2700,  1.5498, -0.5912,  3.2030],\n",
      "        [ 1.6944,  1.5039,  1.0345,  3.2819],\n",
      "        [ 1.2198,  0.8282, -3.3731, -1.0712],\n",
      "        [ 0.2552, -3.1542, -1.5566,  0.5375],\n",
      "        [-0.9542,  0.1490, -1.4006, -1.5407],\n",
      "        [-1.3326,  3.3476, -3.4974, -0.2168],\n",
      "        [ 3.8500,  2.9250,  4.0575,  1.1160]])\n",
      "net.block0.0.bias tensor([0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "net.block0.1.weight tensor([[ 0.9468, -1.1431, -0.3274, -0.0768,  1.9253, -0.1021,  3.0728,  1.1536],\n",
      "        [ 1.5374,  2.3898, -0.9710, -1.3642, -0.1980,  2.2700,  2.9126,  0.0541],\n",
      "        [ 4.2933,  1.1782, -1.9459,  2.9026,  1.7900, -0.5310,  3.5670, -0.2833],\n",
      "        [ 1.0735,  2.5624, -2.5803,  0.9703,  3.4592, -1.4652, -1.0276, -0.4891]])\n",
      "net.block0.1.bias tensor([0., 0., 0., 0.])\n",
      "net.block1.0.weight tensor([[ 1.9721,  1.0801,  2.3878,  2.0278],\n",
      "        [ 1.4146, -0.6792,  0.9853, -1.9449],\n",
      "        [-3.4951,  0.9761, -1.3226,  0.9195],\n",
      "        [ 0.8351,  4.3969, -3.2602, -0.6128],\n",
      "        [-1.9474,  2.1759,  3.9882,  4.8563],\n",
      "        [ 6.0668,  1.2206, -1.8117,  0.5391],\n",
      "        [ 4.2716,  0.5969, -1.6634,  0.3269],\n",
      "        [ 2.9877,  3.8362, -1.4257,  2.8337]])\n",
      "net.block1.0.bias tensor([0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "net.block1.1.weight tensor([[ 1.1281, -0.1552,  0.5101,  1.1790, -2.3145, -0.5395,  0.5185,  0.7824],\n",
      "        [ 0.7443, -0.2312, -0.6808, -0.2809, -1.5623,  1.5816,  0.3119,  3.2536],\n",
      "        [ 5.8048,  3.7259,  0.7810,  0.0764,  3.4103, -0.2782,  1.5893,  4.7372],\n",
      "        [ 4.1351,  0.4357,  3.6148,  0.3230,  0.4127, -1.2640,  1.0779,  0.0176]])\n",
      "net.block1.1.bias tensor([0., 0., 0., 0.])\n",
      "net.block2.0.weight tensor([[ 0.7923,  5.9564,  0.9559,  2.0851],\n",
      "        [ 0.2524, -2.5651,  4.4343,  1.7164],\n",
      "        [ 0.3772, -1.4999,  1.0204,  2.0433],\n",
      "        [ 3.4034, -4.8226,  2.2036,  2.4340],\n",
      "        [ 1.1026,  2.6352, -1.0874,  0.2276],\n",
      "        [-1.3527,  0.4652,  1.0260,  0.0878],\n",
      "        [ 2.3045,  0.6138,  1.7773,  2.8326],\n",
      "        [-0.6983,  0.4828,  0.0685,  0.8821]])\n",
      "net.block2.0.bias tensor([0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "net.block2.1.weight tensor([[ 3.8439, -3.0039,  3.6458,  0.5552,  3.1924, -1.1857,  1.7906, -0.9401],\n",
      "        [ 3.6908,  0.1392,  1.0162,  2.1143,  4.6908, -0.4122, -0.1757,  1.5327],\n",
      "        [ 4.6030,  1.6680,  1.2110,  1.5819,  0.5152,  0.9527,  0.7172, -1.1985],\n",
      "        [-2.7333,  3.2475,  1.6918,  0.1374,  0.3608,  0.5207, -0.8792,  1.3248]])\n",
      "net.block2.1.bias tensor([0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "initNet = InitNet(init=True)\n",
    "for name, param in initNet.named_parameters():\n",
    "    print(name, param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa07ad60-0b17-4ead-b2a5-b0adc4a6db32",
   "metadata": {},
   "source": [
    "2. 查看初始化模块文档以了解不同的初始化方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca5e679-84fd-4b10-a579-9464c1a8053b",
   "metadata": {},
   "source": [
    "+ calculate_gain()\n",
    "+ uniform_()\n",
    "+ normal_()\n",
    "+ constant_()\n",
    "+ ones_()\n",
    "+ zeros_()\n",
    "+ eye_()\n",
    "+ dirac_()\n",
    "+ xavier_uniform_()\n",
    "+ xavier_normal_()\n",
    "+ kaiming_uniform_()\n",
    "+ kaiming_normal_()\n",
    "+ trunc_normal_()\n",
    "+ orthogonal_()\n",
    "+ sparse_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1bc994-6b3b-4370-aba8-cd9f2f3bbf44",
   "metadata": {},
   "source": [
    "3. 构建包含共享参数层的多层感知机并对其进行训练。在训练过程中，观察模型各层的参数和梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51badb43-6bc7-4d00-997c-e044d6916479",
   "metadata": {},
   "source": [
    "4. 为什么共享参数是个好主意？"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
